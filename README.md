# Awesome-Healthcare-Foundation-Models

Curated list of awesome large AI models (LAMs), or foundation models, in healthcare. We organize the current LAMs into four categories: large language models (LLMs), large vision models (LVMs), large audio models (LAudiMs), and large multi-modal models (LMMs). The areas that these LAMs are applied to include but not limited to bioinformatics, medical diagnosis and decision making, medical imaging and vision, medical informatics, medical education, public health, and medical robotics.

We welcome contributions to this repository to add more resources. Please submit a pull request if you want to contribute!

## Survey

This repository is largely based on the following paper:

**[Large AI Models in Health Informatics:
Applications, Challenges, and the Future](https://arxiv.org/pdf/2303.11568v1.pdf)**

If you find this repository helpful, please consider citing:

```
@article{qiu2023large,
  title={Large AI Models in Health Informatics: Applications, Challenges, and the Future},
  author={Qiu, Jianing and Li, Lin and Sun, Jiankai and Peng, Jiachuan and Shi, Peilun and Zhang, Ruiyang and Dong, Yinzhao and Lam, Kyle and Lo, Frank P-W and Xiao, Bo and others},
  journal={arXiv preprint arXiv:2303.11568},
  year={2023}
}

```

## Large Language Models

**[ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using
Large Language Models](https://arxiv.org/pdf/2302.07257.pdf)**

**[DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4](https://arxiv.org/pdf/2303.11032.pdf)**

**[Capabilities of GPT-4 on Medical Challenge Problems](https://arxiv.org/pdf/2303.13375.pdf)**

**[BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://arxiv.org/pdf/1901.08746.pdf)**

**[Publicly Available Clinical BERT Embeddings](https://arxiv.org/pdf/1904.03323.pdf)**

**[BioMegatron: Larger Biomedical Domain Language Model](https://arxiv.org/pdf/2010.06060.pdf)**

**[Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks](https://aclanthology.org/2020.acl-main.740.pdf)**

**[Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction](https://www.nature.com/articles/s41746-021-00455-y)**

**[BioELECTRA:Pretrained Biomedical text Encoder using Discriminators](https://aclanthology.org/2021.bionlp-1.16.pdf)**

**[LinkBERT: Pretraining Language Models with Document Links](https://arxiv.org/pdf/2203.15827.pdf)**

**[BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining](https://arxiv.org/pdf/2210.10341.pdf)**

**[Large Language Models Encode Clinical Knowledge](https://arxiv.org/pdf/2212.13138.pdf)**

**[A large language model for electronic health records](https://www.nature.com/articles/s41746-022-00742-2)**

**[Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing](https://arxiv.org/pdf/2007.15779.pdf)**

**[BEHRT: Transformer for Electronic Health Records](https://www.nature.com/articles/s41598-020-62922-y)**







## Large Vision Models






## Large Audio Models



## Large Multi-modal Models

**[GPT-4 Technical Report](https://arxiv.org/pdf/2303.08774.pdf)**

**[Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning](https://www.nature.com/articles/s41551-022-00936-9)**

**[Contrastive Learning of Medical Visual Representations from Paired Images and Text](https://arxiv.org/pdf/2010.00747.pdf)**
